{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First CNN for SMILES multilabel classification.\n",
    "\n",
    "I slightly changed approach from the one I mentioned with you, followed in \"Learnign to SMILES\" paper, because Keras manage text input in a different way.\n",
    "\n",
    "Here's the code and the explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalMaxPooling1D, Dropout, Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "\n",
    "# The default Tensorflow behavior is to allocate memory on all the available GPUs, even if it runs only on the selected\n",
    "# one. To avoid it, only the selceted GPU (selected by cmd line input) is made visible\n",
    "gpu = str(sys.argv[1])\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "# For allocating memory gradually as it is needed \n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data previously transformed and saved - list of SMILES strings and labels \"matrix\": each labels is a 213-dimensional vector with 1 at the indices of the associated terms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LOC = '../data/'\n",
    "smiles = np.load(DATA_LOC+'smiles.npy')\n",
    "y = np.load(DATA_LOC+'multi_labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text transformation: each string is tokenized at char level, with the fit_on_text method a vocabulary for mapping char to integer indices is learned, and finally the strings are transformed to sequences of integers and padded with 0 at the end.\n",
    "\n",
    "Data are then splitted in training/test set. This is just for a first evaluation of the model, a more accurate evaluation should be performed using k-fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples:  8268\n",
      "Number of test examples:  2068\n",
      "Multi-label classification, number of labels:  213\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer(filters='', lower=False, char_level=True)\n",
    "t.fit_on_texts(smiles)\n",
    "seqs = t.texts_to_sequences(smiles)\n",
    "X = pad_sequences(seqs)\n",
    "\n",
    "# Split in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print('Number of training examples: ', X_train.shape[0])\n",
    "print('Number of test examples: ', X_test.shape[0])\n",
    "print('Multi-label classification, number of labels: ', y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "sequence_length = X.shape[1]\n",
    "vocabulary_size = len(t.word_index)\n",
    "n_class = y_train.shape[1]\n",
    "embedding_size = 32\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what I've seen, this is how integers sequences representing text are usually managed in Keras. The embedding layer basically turns the positive integers into dense vectors of fixed size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(output_dim=embedding_size, input_dim=vocabulary_size,\n",
    "                    input_length=sequence_length))\n",
    "model.add(Convolution1D(32, 2, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Convolution1D(32, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(n_class, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what I have understood reading about this, in multi-label classification problems the top layer should have sigmoid activation instead of softmax, in order to predict the probabilities for each node (each label) indipendently.\n",
    "\n",
    "Also binary crossentropy and sgd optimizer are indicated for this type of problem, but now I've noticed that Adam optimizer outperformed sgd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high accuracy value I got was probably due (according to some issue comments found on GitHub) to the sparse labels vectors, so that there was an high number of correctly predicted 0s pushing the accuracy to 0.99, using the standard \"evaluate\" method of Keras model.\n",
    "\n",
    "I found [this example](https://github.com/suraj-deshmukh/Multi-Label-Image-Classification/blob/master/miml.ipynb \"Multi-Label-Image-Classification\") where they use this approach for having a more reliable accuracy measure. I'd like to know if you agree with this or not.\n",
    "\n",
    "Basically, the predicted labels has to be obtained thresholding the probabilities computed by the output sigmoid layer. This threshold can be fixed (e.g. 0.5) or can be adapted for each label, choosing the threshold that gives greater MCC value. I got better accuracy values \"tuning\" different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.predict(X_test)\n",
    "out = np.array(out, dtype=np.float32)\n",
    "\n",
    "# # Thresholdin probabilites at 0.5\n",
    "# y_pred = np.zeros(out.shape)\n",
    "# y_pred[np.where(out>=0.5)] = 1\n",
    "\n",
    "# # Thresholding probabilities adapting the threshold for each label\n",
    "threshold = np.arange(0.1,1,0.1)\n",
    "acc = []\n",
    "accuracies = []\n",
    "best_threshold = np.zeros(out.shape[1])\n",
    "\n",
    "for i in range(out.shape[1]):\n",
    "    y_prob = np.array(out[:,i])\n",
    "    for j in threshold:\n",
    "        y_pred = [1 if prob>=j else 0 for prob in y_prob]\n",
    "        acc.append(matthews_corrcoef(y_test[:,i], y_pred))\n",
    "    acc = np.array(acc)\n",
    "    index = np.where(acc==acc.max()) \n",
    "    accuracies.append(acc.max()) \n",
    "    best_threshold[i] = threshold[index[0][0]]\n",
    "    acc = []\n",
    "y_pred = np.array([[1 if out[i,j]>=best_threshold[j] else 0 for j\\\n",
    "                    in range(y_test.shape[1])] for i in range(len(y_test))])\n",
    "total_correctly_predicted = len([i for i in range(len(y_test)) if (y_test[i]==y_pred[i]).sum() == n_class])\n",
    "\n",
    "print('hamming_loss: ', hamming_loss(y_test, y_pred))\n",
    "print('Acc: ', str(total_correctly_predicted/y_test.shape[0]))\n",
    "print('total_correctly_predicted: ', total_correctly_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hamming_loss:  0.006002488172101597\n",
    "\n",
    "Acc:  0.3176982591876209\n",
    "\n",
    "total_correctly_predicted:  657 (out of 2019 examples in test set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
