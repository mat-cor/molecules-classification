{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First CNN for SMILES multilabel classification.\n",
    "\n",
    "I slightly changed approach from the one I mentioned with you, followed in \"Learnign to SMILES\" paper, because Keras manage text input in a different way.\n",
    "\n",
    "Here's the code and the explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalMaxPooling1D, Dropout, Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "\n",
    "# The default Tensorflow behavior is to allocate memory on all the available GPUs, even if it runs only on the selected\n",
    "# one. To avoid it, only the selceted GPU (selected by cmd line input) is made visible\n",
    "gpu = str(sys.argv[1])\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "# For allocating memory gradually as it is needed \n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data previously transformed and saved - list of SMILES strings and labels \"matrix\": each labels is a 213-dimensional vector with 1 at the indices of the associated terms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LOC = '../data/'\n",
    "smiles = np.load(DATA_LOC+'smiles.npy')\n",
    "y = np.load(DATA_LOC+'multi_labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text transformation: each string is tokenized at char level, with the fit_on_text method a vocabulary for mapping char to integer indices is learned, and finally the strings are transformed to sequences of integers and padded with 0 at the end.\n",
    "\n",
    "Data are then splitted in training/test set. This is just for a first evaluation of the model, a more accurate evaluation should be performed using k-fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples:  8268\n",
      "Number of test examples:  2068\n",
      "Multi-label classification, number of labels:  213\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer(filters='', lower=False, char_level=True)\n",
    "t.fit_on_texts(smiles)\n",
    "seqs = t.texts_to_sequences(smiles)\n",
    "X = pad_sequences(seqs)\n",
    "\n",
    "# Split in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print('Number of training examples: ', X_train.shape[0])\n",
    "print('Number of test examples: ', X_test.shape[0])\n",
    "print('Multi-label classification, number of labels: ', y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "sequence_length = X.shape[1]\n",
    "vocabulary_size = len(t.word_index)\n",
    "n_class = y_train.shape[1]\n",
    "embedding_size = 32\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what I've seen this is how integers sequences representing text are usually managed in Keras. The embedding layer basically turns the positive integers into dense vectors of fixed size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(output_dim=embedding_size, input_dim=vocabulary_size, input_length=sequence_length))\n",
    "model.add(Convolution1D(64, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution1D(64, 3, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this level I tried using either MaxPooling and Global Pooling, with the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this Dense layer could be our fingerprint, so I set its size to 1024 (usual size for fingerprints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what I have understood reading about this, in multi-label classification problems the top layer should have sigmoid activation instead of softmax, in order to predict the probabilities for each node (each label) indipendently.\n",
    "\n",
    "Also binary crossentropy and sgd optimizer are indicated for this type of problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(n_class, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "score = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print('Test score: ', model.metrics_names, score)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(I trained the model with GPU on Titan server, so I'll just paste the output here)\n",
    "\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #\n",
    "=================================================================\n",
    "embedding_1 (Embedding)      (None, 1021, 32)          1792\n",
    "_________________________________________________________________\n",
    "conv1d_1 (Conv1D)            (None, 1019, 64)          6208\n",
    "_________________________________________________________________\n",
    "max_pooling1d_1 (MaxPooling1 (None, 509, 64)           0\n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 509, 64)           0\n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 507, 64)           12352\n",
    "_________________________________________________________________\n",
    "global_max_pooling1d_1 (Glob (None, 64)                0\n",
    "_________________________________________________________________\n",
    "dropout_2 (Dropout)          (None, 64)                0\n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 1024)              66560\n",
    "_________________________________________________________________\n",
    "dropout_3 (Dropout)          (None, 1024)              0\n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 213)               218325\n",
    "=================================================================\n",
    "Total params: 305,237\n",
    "Trainable params: 305,237\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "None\n",
    "Epoch 1/10\n",
    "8268/8268 [==============================] - 4s 445us/step - loss: 0.6905 - acc: 0.6276\n",
    "Epoch 2/10\n",
    "8268/8268 [==============================] - 3s 330us/step - loss: 0.6849 - acc: 0.8244\n",
    "Epoch 3/10\n",
    "8268/8268 [==============================] - 3s 328us/step - loss: 0.6791 - acc: 0.8994\n",
    "Epoch 4/10\n",
    "8268/8268 [==============================] - 3s 332us/step - loss: 0.6725 - acc: 0.9237\n",
    "Epoch 5/10\n",
    "8268/8268 [==============================] - 3s 323us/step - loss: 0.6644 - acc: 0.9303\n",
    "Epoch 6/10\n",
    "8268/8268 [==============================] - 3s 329us/step - loss: 0.6524 - acc: 0.9285\n",
    "Epoch 7/10\n",
    "8268/8268 [==============================] - 3s 329us/step - loss: 0.6297 - acc: 0.9269\n",
    "Epoch 8/10\n",
    "8268/8268 [==============================] - 3s 329us/step - loss: 0.5708 - acc: 0.9412\n",
    "Epoch 9/10\n",
    "8268/8268 [==============================] - 3s 324us/step - loss: 0.3781 - acc: 0.9750\n",
    "Epoch 10/10\n",
    "8268/8268 [==============================] - 3s 330us/step - loss: 0.1237 - acc: 0.9919\n",
    "2068/2068 [==============================] - 0s 100us/step\n",
    "\n",
    "Test score:  ['loss', 'acc'] [0.14251235412790419, 0.99233795826393589]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some considerations:\n",
    "I tried to change some parameters, for example the number of filters of the conv layers, the embedding size or using Max Pooling instead of Global Max, but I always get this high accuracy value (always 0.99), that makes me think of overfitting. I don't know if it is because there's something wrong in the network architecture, but if this is ok I'd like to ask your opinion about this points:\n",
    "\n",
    " The maximum SMILES length is 1021, that means that all the sequences are padded to this length. I don't think this is good, since the average length of the strings is about 60 and there are also very short strings, but I don't know if it could be better to discard SMILES greather than a certain length or just truncate them (but I suppose that this would lead to have duplicated SMILES again)\n",
    " \n",
    " The approach followed in the paper, where they say that they don't use padding, seems impossible to implement in Keras, because they just say that they manage it fixing the number of pooling unit and dynamically determining the size of the pooling window basing on the sequence length. This is not very clear to me, but anyway I don't think it's possible to manage input of different lengths since the Embedding and the Convolutional layers require fixed input size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
